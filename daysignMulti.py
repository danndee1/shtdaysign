import os
import re
import time
import httpx
import traceback
import random
import math
from datetime import datetime, timedelta  # æ­£ç¡®å¯¼å…¥ datetime å’Œ timedelta
from contextlib import contextmanager
from bs4 import BeautifulSoup
from xml.etree import ElementTree as ET
from flaresolverr import FlareSolverrHTTPClient

SEHUATANG_HOST = 'www.sehuatang.net'
DEFAULT_USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'

FID = 103  # é«˜æ¸…ä¸­æ–‡å­—å¹•

AUTO_REPLIES = (
    'æ„Ÿè°¢æ¥¼ä¸»åˆ†äº«å¥½ç‰‡',
    'æ„Ÿè°¢åˆ†äº«ï¼ï¼',
    'æ„Ÿè°¢åˆ†äº«æ„Ÿè°¢åˆ†äº«',
    'å¿…éœ€æ”¯æŒ',
    'ç®€ç›´å¤ªçˆ½äº†',
    'æ„Ÿè°¢åˆ†äº«å•Š',
    'å°é¢è¿˜ä¸é”™',
    'æœ‰ç‚¹æ„æ€å•Š',
    'å°é¢è¿˜ä¸é”™ï¼Œæ”¯æŒä¸€æ³¢',
    'çœŸä¸é”™å•Š',
    'ä¸é”™ä¸é”™',
    'è¿™èº«æå¯ä»¥å‘€',
    'ç»ˆäºç­‰åˆ°ä½ ',
    'ä¸é”™ã€‚ã€‚ï¼',
    'è¬è¬è¾›è‹¦åˆ†äº«',
    'èµå¿ƒæ‚¦ç›®',
    'å¿«ä¹æ— é™~~',
    'é€™æ€éº¼å—çš„äº†å•Š',
    'è°ä¹ŸæŒ¡ä¸ä½ï¼',
    'æ„Ÿè¬åˆ†äº«',
    'åˆ†äº«æ”¯æŒã€‚',
    'è¿™è°é¡¶å¾—ä½å•Š',
    'è¿™æ˜¯è¦ç²¾Jäººäº¡å•Šï¼',
    'é¥°æ¼”å¾ˆèµ',
    'é€™ç³»åˆ—çœŸæœ‰æˆ²',
    'æ„Ÿè°¢å¤§ä½¬åˆ†äº«',
    'çœ‹ç€ä¸é”™',
    'æ„Ÿè°¢è€æ¿åˆ†äº«',
    'å¯ä»¥çœ‹çœ‹',
    'è°¢è°¢åˆ†äº«ï¼ï¼ï¼',
    'éå¸¸æ„Ÿè°¢ï¼',
    'æ¥¼ä¸»å¨æ­¦ï¼',
    'è¾›è‹¦äº†ï¼Œè°¢è°¢åˆ†äº«',
    'åˆ†äº«æ„‰å¿«',
    'å¥½ç‰‡ï¼Œæ”¶è—äº†',
    'å¤šè°¢åˆ†äº«',
    'æœŸå¾…æ›´å¤šå¥½ä½œå“',
    'æ¥¼ä¸»ç»™åŠ›',
    'å¾ˆæ£’ï¼Œæ”¯æŒä¸€ä¸‹',
    'å¥½äººä¸€ç”Ÿå¹³å®‰',
    'è†œæ‹œå¤§ç¥',
    'éå¸¸æ£’çš„èµ„æº',
    'è¿™ç‰‡å­çœŸä¸é”™',
    'å€¼å¾—ä¸€çœ‹',
    'å¥½ä¸œè¥¿ï¼Œæ”¯æŒ',
    'ä¸‹è½½çœ‹çœ‹',
    'æ¥¼ä¸»è¾›è‹¦äº†',
    'å¤šè°¢æ¥¼ä¸»',
    'ç›¸å½“ä¸é”™',
    'å‰å®³äº†ï¼Œæ„Ÿè°¢',
    'ä¼˜ç§€çš„èµ„æº'
)


def daysign(
    cookies: dict,
    flaresolverr_url: str = None,
    flaresolverr_proxy: str = None,
    proxies: dict = None  # æ–°å¢çš„ proxies å‚æ•°ï¼Œç”¨äºä¼ å…¥ä»£ç†é…ç½®
) -> str:
   # å¦‚æœä¼ å…¥äº†ä»£ç†ï¼Œåˆ™è¾“å‡ºä½¿ç”¨çš„ä»£ç†
    if proxies:
        print(f"Using proxy: {proxies}")
    else:
        print("No proxy configured.")

    with (FlareSolverrHTTPClient(url=flaresolverr_url,
                                 proxy=flaresolverr_proxy,
                                 cookies=cookies,
                                 http2=True)
          if flaresolverr_url else httpx.Client(cookies=cookies, http2=True, proxies=proxies)) as client:

        @contextmanager
        def _request(method, url, *args, **kwargs):
            r = client.request(method=method, url=url,
                               headers={
                                   'user-agent': DEFAULT_USER_AGENT,
                                   'x-requested-with': 'XMLHttpRequest',
                                   'dnt': '1',
                                   'accept': '*/*',
                                   'sec-ch-ua-mobile': '?0',
                                   'sec-ch-ua-platform': 'macOS',
                                   'sec-fetch-site': 'same-origin',
                                   'sec-fetch-mode': 'cors',
                                   'sec-fetch-dest': 'empty',
                                   'referer': f'https://{SEHUATANG_HOST}/plugin.php?id=dd_sign&mod=sign',
                                   'accept-language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                               }, *args, **kwargs)
            try:
                r.raise_for_status()
                yield r
            finally:
                r.close()

        age_confirmed = False
        age_retry_cnt = 3
        while not age_confirmed and age_retry_cnt > 0:
            with _request(method='get', url=f'https://{SEHUATANG_HOST}/') as r:
                if (v := re.findall(r"safeid='(\w+)'",
                                    r.text, re.MULTILINE | re.IGNORECASE)) and (safeid := v[0]):
                    print(f'set age confirm cookie: _safe={safeid}')
                    client.cookies.set(name='_safe', value=safeid)
                else:
                    age_confirmed = True
                age_retry_cnt -= 1

        if not age_confirmed:
            raise Exception('failed to pass age confirmation')

        # éšæœºç­‰å¾…
        wait_time = random.randint(1, 3)
        print(f'åœ¨é€‰æ‹©é¢˜ç›®ä¹‹å‰ç­‰å¾… {wait_time} ç§’...')
        time.sleep(wait_time)
        # éšæœºç­‰å¾…

        with _request(method='get', url=f'https://{SEHUATANG_HOST}/forum.php?mod=forumdisplay&fid={FID}') as r:
            tids = re.findall(r"normalthread_(\d+)", r.text,
                              re.MULTILINE | re.IGNORECASE)
            tid = random.choice(tids)
            print(f'choose tid = {tid} to comment')

        # éšæœºç­‰å¾…
        wait_time = random.randint(1, 3)
        print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
        time.sleep(wait_time)
        # éšæœºç­‰å¾…

        with _request(method='get', url=f'https://{SEHUATANG_HOST}/forum.php?mod=viewthread&tid={tid}&extra=page%3D1') as r:
            soup = BeautifulSoup(r.text, 'html.parser')
            formhash = soup.find('input', {'name': 'formhash'})['value']

        message = random.choice(AUTO_REPLIES)

        # éšæœºç­‰å¾…
        wait_time = random.randint(1, 3)
        print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
        time.sleep(wait_time)
        # éšæœºç­‰å¾…
        with _request(method='post', url=f'https://{SEHUATANG_HOST}/forum.php?mod=post&action=reply&fid={FID}&tid={tid}&extra=page%3D1&replysubmit=yes&infloat=yes&handlekey=fastpost&inajax=1',
                      data={
                          'file': '',
                          'message': message,
                          'posttime': int(time.time()),
                          'formhash': formhash,
                          'usesig': '',
                          'subject': '',
                      }) as r:
            print(f'comment to: tid = {tid}, message = {message}')

        with _request(method='get', url=f'https://{SEHUATANG_HOST}/plugin.php?id=dd_sign&mod=sign') as r:
            # id_hash_rsl = re.findall(
            #     r"updatesecqaa\('(.*?)'", r.text, re.MULTILINE | re.IGNORECASE)
            # id_hash = id_hash_rsl[0] if id_hash_rsl else 'qS0'  # default value

            # soup = BeautifulSoup(r.text, 'html.parser')
            # formhash = soup.find('input', {'name': 'formhash'})['value']
            # signtoken = soup.find('input', {'name': 'signtoken'})['value']
            # action = soup.find('form', {'name': 'login'})['action']
            pass

        # éšæœºç­‰å¾…
        wait_time = random.randint(1, 3)
        print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
        time.sleep(wait_time)
        # éšæœºç­‰å¾…
        with _request(method='get', url=f'https://{SEHUATANG_HOST}/plugin.php?id=dd_sign&ac=sign&infloat=yes&handlekey=pc_click_ddsign&inajax=1&ajaxtarget=fwin_content_pc_click_ddsign') as r:
            soup = BeautifulSoup(r.text, 'xml')
            html = soup.find('root').string
            # extract argument values from signform
            root = BeautifulSoup(html, 'html.parser')
            id_hash = (root.find('span', id=re.compile(r'^secqaa_'))
                       ['id']).removeprefix('secqaa_')
            formhash = root.find('input', {'name': 'formhash'})['value']
            signtoken = root.find('input', {'name': 'signtoken'})['value']
            action = root.find('form', {'name': 'login'})['action']
            print(
                f'signform values: id_hash={id_hash}, formhash={formhash}, signtoken={signtoken}')
            print(f'action href: {action}')

        # GET: https://www.sehuatang.net/misc.php?mod=secqaa&action=update&idhash=qS0&0.2010053552105764
        with _request(method='get', url=f'https://{SEHUATANG_HOST}/misc.php?mod=secqaa&action=update&idhash={id_hash}&{round(random.random(), 16)}') as r:
            qes_rsl = re.findall(r"'(.*?) = \?'", r.text,
                                 re.MULTILINE | re.IGNORECASE)

            if not qes_rsl or not qes_rsl[0]:
                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤ç­¾åˆ°
                if 'é‡å¤ç­¾åˆ°' in r.text:
                    return 'é‡å¤ç­¾åˆ°'
                raise Exception('invalid or empty question!')
            qes = qes_rsl[0]
            ans = eval(qes)
            print(f'verification question: {qes} = {ans}')
            assert type(ans) == int

        # éšæœºç­‰å¾…
        wait_time = random.randint(1, 3)
        print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
        time.sleep(wait_time)
        # éšæœºç­‰å¾…
        # POST: https://www.sehuatang.net/plugin.php?id=dd_sign&mod=sign&signsubmit=yes&signhash=LMAB9&inajax=1
        with _request(method='post', url=f'https://{SEHUATANG_HOST}/{action.lstrip("/")}&inajax=1',
                      data={'formhash': formhash,
                            'signtoken': signtoken,
                            'secqaahash': id_hash,
                            'secanswer': ans}) as r:
            if 'ç­¾åˆ°æˆåŠŸ' in r.text:
                output = f"ç­¾åˆ°æˆåŠŸ\n\n"
                #è·å–ä¸ªäººè´¦å·å
                with _request(method='get', url=f'https://{SEHUATANG_HOST}/home.php?mod=spacecp&ac=profile') as r:
                    soup = BeautifulSoup(r.text, 'html.parser')
                    # æŸ¥æ‰¾æ ‡ç­¾
                    username_element = soup.find('strong', class_='vwmy')
                    # æå–ç”¨æˆ·ä¿¡æ¯
                    if username_element:
                        username = username_element.text
                        # æ©ç å¤„ç†ï¼šä¿ç•™é¦–å­—æ¯å’Œæœ€åä¸€å­—æ¯ï¼Œå…¶ä»–éƒ¨åˆ†ç”¨æ˜Ÿå·ä»£æ›¿
                        def mask_username(username):
                            if len(username) <= 2:
                                # å¦‚æœç”¨æˆ·ååªæœ‰ä¸€ä¸¤ä½ï¼Œæ©ç ä¸€åŠæˆ–å®Œå…¨æ©ç›–
                                return username[0] + '*' * (len(username) - 1)
                            else:
                                return username[0] + '*' * (len(username) - 2) + username[-1]
                        masked_username = mask_username(username)
                        print(f"ç”¨æˆ·å: {masked_username}")
                    else:
                        print("æœªæ‰¾åˆ°ç”¨æˆ·å")
                    # è¾“å‡ºç»“æœ
                    output += f"ç”¨æˆ·å: {masked_username} \n"
                #è·å–é‡‘å¸ç­‰ä¿¡æ¯
                with _request(method='get', url=f'https://{SEHUATANG_HOST}/home.php?mod=spacecp&ac=credit&showcredit=1') as r:
                    soup = BeautifulSoup(r.text, 'html.parser')
                    # æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰ <li> æ ‡ç­¾çš„ç§¯åˆ†ä¿¡æ¯
                    credit_info = soup.find_all('li')
                    # æå–ç§¯åˆ†å’Œé‡‘é’±ç­‰ä¿¡æ¯
                    credits = {}
                    first_integral_found = False
                    for item in credit_info:
                        if item.find('em'):  # æ£€æŸ¥æ˜¯å¦æœ‰ <em> æ ‡ç­¾
                            key = item.find('em').text.strip().replace(':', '')
                            value = item.text.split(":")[-1].strip()
                            if key == "ç§¯åˆ†" and not first_integral_found:
                                credits[key] = value
                                first_integral_found = True
                            elif key!= "ç§¯åˆ†":
                                credits[key] = value
                    # è·å–ç”¨æˆ·ç»„ä¿¡æ¯
                    user_group_tag = soup.find('a', {'id': 'g_upmine'})
                    user_group = user_group_tag.text.split(":")[-1].strip() if user_group_tag else 'æœªæ‰¾åˆ°'
                    # è·å–å„ç±»ç§¯åˆ†ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œè¾“å‡ºæ ¼å¼åŒ–
                    gold = credits.get('é‡‘é’±', 'æœªæ‰¾åˆ°')
                    points = credits.get('ç§¯åˆ†', 'æœªæ‰¾åˆ°')
                    coins = credits.get('è‰²å¸', 'æœªæ‰¾åˆ°')
                    ratings = credits.get('è¯„åˆ†', 'æœªæ‰¾åˆ°')
                    # è¾“å‡ºç»“æœ
                    output += f"ğŸ¤¼â€â™‚ï¸ç”¨æˆ·ç»„ï¼š{user_group} \nğŸ’°é‡‘å¸ï¼š{gold}\nğŸ¥‡ç§¯åˆ†ï¼š{points}\nğŸ˜è‰²å¸ï¼š{coins}\nğŸ”¥è¯„åˆ†ï¼š{ratings}\n"
                #è·å–å‡çº§è¯¦æƒ…
                # éšæœºç­‰å¾…
                wait_time = random.randint(1, 3)
                print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
                time.sleep(wait_time)
                # éšæœºç­‰å¾…
                with _request(method='get', url=f'https://{SEHUATANG_HOST}/home.php?mod=spacecp&ac=usergroup') as r:
                    soup = BeautifulSoup(r.text, 'html.parser')
                    # è·å–æ™‹çº§ç”¨æˆ·ç»„ä¿¡æ¯
                    upgrade_usergroup = soup.find('li', {'id': 'c2'}).text
                    #print("æ™‹çº§ç”¨æˆ·ç»„ä¿¡æ¯:", upgrade_usergroup)
                    # æŸ¥æ‰¾åŒ…å«æ™‹çº§ç”¨æˆ·ç»„ä¿¡æ¯çš„div
                    tscr_div = soup.find('div', class_='tscr')
                    if tscr_div:
                        # æå–å‡çº§æ‰€éœ€ç§¯åˆ†ä¿¡æ¯
                        required_points = tscr_div.find('span', class_='notice').text
                        # ä»æ–‡æœ¬ä¸­æå–ç§¯åˆ†æ•°å­—
                        points_needed = int(required_points.split('ç§¯åˆ†')[1].strip())
                        # è®¡ç®—é¢„è®¡å‡çº§æ—¶é—´
                        points_per_day = 3
                        days_needed = points_needed / points_per_day
                        # æ‰‹åŠ¨å‘ä¸Šå–æ•´
                        if days_needed != int(days_needed):
                            days_needed_ceil = int(days_needed) + 1
                        else:
                            days_needed_ceil = int(days_needed)
                        # è®¡ç®—é¢„è®¡å‡çº§æ—¥æœŸ
                        today = datetime.now()
                        todayDate = today.date()  
                        upgrade_date = todayDate + timedelta(days=days_needed_ceil)
                    else:
                        print("æœªæ‰¾åˆ°åŒ…å«æ™‹çº§ç”¨æˆ·ç»„ä¿¡æ¯çš„ divã€‚")
                    # è¾“å‡ºç»“æœ
                    output += f"\n{upgrade_usergroup} \n{required_points}\n"
                    output += f"é¢„è®¡è¿˜éœ€{days_needed_ceil}å¤©\n"
                    output += f"é¢„è®¡å‡çº§æ—¶é—´: {upgrade_date}"
                return output
            elif 'é‡å¤ç­¾åˆ°' in r.text:
                return 'é‡å¤ç­¾åˆ°'
            elif 'éœ€è¦å…ˆç™»å½•' in r.text:
                return 'éœ€è¦å…ˆç™»å½•'
            else:
                return r.text

def retrieve_cookies_from_curl(env: str) -> dict:
    cURL = os.getenv(env, '').replace('\\', ' ')
    try:
        import uncurl
        return uncurl.parse_context(curl_command=cURL).cookies
    except ImportError:
        print("uncurl is required.")


def retrieve_cookies_from_fetch(env: str) -> dict:
    def parse_fetch(s: str) -> dict:
        ans = {}
        exec(s, {
            'fetch': lambda _, o: ans.update(o),
            'null': None
        })
        return ans
    cookie_str = parse_fetch(os.getenv(env))['headers']['cookie']
    return dict(s.strip().split('=', maxsplit=1) for s in cookie_str.split(';'))


def preprocess_text(text) -> str:
    if 'xml' not in text:
        return text

    try:
        root = ET.fromstring(text)
        cdata = root.text
        soup = BeautifulSoup(cdata, 'html.parser')
        for script in soup.find_all('script'):
            script.decompose()
        return soup.get_text()
    except:
        return text


def push_notification(title: str, content: str) -> None:

    def telegram_send_message(text: str, chat_id: str, token: str, silent: bool = False) -> None:
        r = httpx.post(url=f'https://api.telegram.org/bot{token}/sendMessage',
                       json={
                           'chat_id': chat_id,
                           'text': text,
                           'disable_notification': silent,
                           'disable_web_page_preview': True,
                       })
        r.raise_for_status()

    try:
        from notify import telegram_bot
        telegram_bot(title, content)
    except ImportError:
        chat_id = os.getenv('TG_USER_ID')
        bot_token = os.getenv('TG_BOT_TOKEN')
        if chat_id and bot_token:
            telegram_send_message(f'{title}\n\n{content}', chat_id, bot_token)


def main():
    raw_html = None
    results = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰ç­¾åˆ°ç»“æœ

    # éå†å¤šä¸ª FETCH_98TANG ç¯å¢ƒå˜é‡
    fetch_index = 1
    while os.getenv(f'FETCH_98TANG_{fetch_index}'):
        # è·å– cookies
        cookies = retrieve_cookies_from_fetch(f'FETCH_98TANG_{fetch_index}')
        
        # è·å–å¯¹åº”çš„ä»£ç†
        proxy = os.getenv(f'DEAULT_PROXY_{fetch_index}', None)
        if proxy:
            proxies = {'http://': proxy, 'https://': proxy}
            print(f"Using proxy for FETCH_98TANG_{fetch_index}: {proxies}")
        else:
            proxies = None
            print(f"No proxy configured for FETCH_98TANG_{fetch_index}")

        try:
            # æ‰§è¡Œç­¾åˆ°ï¼Œå¹¶ä¼ é€’ä»£ç†ä¿¡æ¯
            raw_html = daysign(
                cookies=cookies,
                flaresolverr_url=os.getenv('FLARESOLVERR_URL'),
                flaresolverr_proxy=os.getenv('FLARESOLVERR_PROXY'),
                proxies=proxies  # å°†ä»£ç†ä¼ é€’ç»™ daysign å‡½æ•°
            )

            if 'ç­¾åˆ°æˆåŠŸ' in raw_html:
                title = f'ç¬¬{fetch_index}ä¸ªè´¦å·'
                message_text = raw_html
            elif raw_html == 'é‡å¤ç­¾åˆ°':
                title = f'ç¬¬{fetch_index}ä¸ªè´¦å·'
                message_text = 'æœ¬æ—¥å·²ç­¾åˆ°ï¼Œè¯·å‹¿é‡å¤ç­¾åˆ°'
            elif raw_html == 'éœ€è¦å…ˆç™»å½•':
                title = f'ç¬¬{fetch_index}ä¸ªè´¦å· ç­¾åˆ°å¼‚å¸¸'
                message_text = 'Cookieæ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·é‡æ–°è·å–'
            else:
                title = f'ç¬¬{fetch_index}ä¸ªè´¦å· ç­¾åˆ°å¼‚å¸¸'
                message_text = raw_html
        except Exception as e:
            title = f'ç¬¬{fetch_index}ä¸ªè´¦å· ç­¾åˆ°å¼‚å¸¸'
            message_text = f'é”™è¯¯åŸå› ï¼š{e}'
            traceback.print_exc()

        # å¤„ç†å¹¶ä¿å­˜ç»“æœ
        message_text = preprocess_text(message_text)
        results.append(f'{title}\n{message_text}')
        
        fetch_index += 1

    # è¾“å‡ºæ‰€æœ‰ç»“æœ
    for result in results:
        print(result)

    #é’é¾™é€šçŸ¥
    full_message = "\n\n".join(results)
    from notify import send
    send('ğŸ—“98å ‚æ¯æ—¥ç­¾åˆ°', full_message)

if __name__ == '__main__':
    main()
