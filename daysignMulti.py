import os
import re
import time
import httpx
import traceback
import random
import math
import configparser
from datetime import datetime, timedelta
from contextlib import contextmanager
from bs4 import BeautifulSoup
from xml.etree import ElementTree as ET
from flaresolverr import FlareSolverrHTTPClient

# åŠ è½½é…ç½®æ–‡ä»¶
config = configparser.ConfigParser()
config.read('config.ini', encoding='utf-8')

# è·å–ä»£ç†æ± é…ç½®
proxy_pool = [proxy.strip() for proxy in config.get('proxy_pool', 'proxies').split('\n') if proxy.strip()]
max_retries = config.getint('retry', 'max_retries')
retry_delay = config.getint('retry', 'retry_delay')

SEHUATANG_HOST = 'www.sehuatang.net'
DEFAULT_USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'

FID = 103  # é«˜æ¸…ä¸­æ–‡å­—å¹•

AUTO_REPLIES = (
    'æ„Ÿè°¢æ¥¼ä¸»åˆ†äº«å¥½ç‰‡',
    'æ„Ÿè°¢åˆ†äº«ï¼ï¼',
    'æ„Ÿè°¢åˆ†äº«æ„Ÿè°¢åˆ†äº«',
    'å¿…éœ€æ”¯æŒ',
    'ç®€ç›´å¤ªçˆ½äº†',
    'æ„Ÿè°¢åˆ†äº«å•Š',
    'å°é¢è¿˜ä¸é”™',
    'æœ‰ç‚¹æ„æ€å•Š',
    'å°é¢è¿˜ä¸é”™ï¼Œæ”¯æŒä¸€æ³¢',
    'çœŸä¸é”™å•Š',
    'ä¸é”™ä¸é”™',
    'è¿™èº«æå¯ä»¥å‘€',
    'ç»ˆäºç­‰åˆ°ä½ ',
    'ä¸é”™ã€‚ã€‚ï¼',
    'è¬è¬è¾›è‹¦åˆ†äº«',
    'èµå¿ƒæ‚¦ç›®',
    'å¿«ä¹æ— é™~~',
    'é€™æ€éº¼å—çš„äº†å•Š',
    'è°ä¹ŸæŒ¡ä¸ä½ï¼',
    'æ„Ÿè¬åˆ†äº«',
    'åˆ†äº«æ”¯æŒã€‚',
    'è¿™è°é¡¶å¾—ä½å•Š',
    'è¿™æ˜¯è¦ç²¾Jäººäº¡å•Šï¼',
    'é¥°æ¼”å¾ˆèµ',
    'é€™ç³»åˆ—çœŸæœ‰æˆ²',
    'æ„Ÿè°¢å¤§ä½¬åˆ†äº«',
    'çœ‹ç€ä¸é”™',
    'æ„Ÿè°¢è€æ¿åˆ†äº«',
    'å¯ä»¥çœ‹çœ‹',
    'è°¢è°¢åˆ†äº«ï¼ï¼ï¼',
    'éå¸¸æ„Ÿè°¢ï¼',
    'æ¥¼ä¸»å¨æ­¦ï¼',
    'è¾›è‹¦äº†ï¼Œè°¢è°¢åˆ†äº«',
    'åˆ†äº«æ„‰å¿«',
    'å¥½ç‰‡ï¼Œæ”¶è—äº†',
    'å¤šè°¢åˆ†äº«',
    'æœŸå¾…æ›´å¤šå¥½ä½œå“',
    'æ¥¼ä¸»ç»™åŠ›',
    'å¾ˆæ£’ï¼Œæ”¯æŒä¸€ä¸‹',
    'å¥½äººä¸€ç”Ÿå¹³å®‰',
    'è†œæ‹œå¤§ç¥',
    'éå¸¸æ£’çš„èµ„æº',
    'è¿™ç‰‡å­çœŸä¸é”™',
    'å€¼å¾—ä¸€çœ‹',
    'å¥½ä¸œè¥¿ï¼Œæ”¯æŒ',
    'ä¸‹è½½çœ‹çœ‹',
    'æ¥¼ä¸»è¾›è‹¦äº†',
    'å¤šè°¢æ¥¼ä¸»',
    'ç›¸å½“ä¸é”™',
    'å‰å®³äº†ï¼Œæ„Ÿè°¢',
    'ä¼˜ç§€çš„èµ„æº'
)


def get_random_proxy():
    """ä»ä»£ç†æ± ä¸­éšæœºè·å–ä¸€ä¸ªä»£ç†"""
    if not proxy_pool:
        return None
    return random.choice(proxy_pool)

def create_proxy_dict(proxy_url):
    """åˆ›å»ºä»£ç†å­—å…¸"""
    if not proxy_url:
        return None
    return {
        'http': proxy_url,
        'https': proxy_url
    }

def daysign(
    cookies: dict,
    flaresolverr_url: str = None,
    flaresolverr_proxy: str = None,
    proxies: dict = None  # æ–°å¢çš„ proxies å‚æ•°ï¼Œç”¨äºä¼ å…¥ä»£ç†é…ç½®
) -> str:
    current_proxy = None
    used_proxies = set()  # è®°å½•å·²ä½¿ç”¨çš„ä»£ç†
    
    # å¦‚æœä¼ å…¥äº†ä»£ç†ï¼Œåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ä»£ç†
    if proxies:
        print(f"ä½¿ç”¨ç¯å¢ƒå˜é‡ä»£ç†: {proxies}")
        current_proxy = proxies
    else:
        # ä»ä»£ç†æ± ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä»£ç†
        proxy_url = get_random_proxy()
        if proxy_url:
            current_proxy = create_proxy_dict(proxy_url)
            print(f"ä½¿ç”¨ä»£ç†æ± ä»£ç†: {current_proxy}")
            used_proxies.add(proxy_url)

    for retry in range(max_retries):
        try:
            client = (FlareSolverrHTTPClient(url=flaresolverr_url,
                                     proxy=flaresolverr_proxy,
                                     cookies=cookies,
                                     http2=True)
                if flaresolverr_url else httpx.Client(cookies=cookies, http2=True, proxies=current_proxy))

            @contextmanager
            def _request(method, url, *args, **kwargs):
                r = client.request(method=method, url=url,
                                   headers={
                                       'user-agent': DEFAULT_USER_AGENT,
                                       'x-requested-with': 'XMLHttpRequest',
                                       'dnt': '1',
                                       'accept': '*/*',
                                       'sec-ch-ua-mobile': '?0',
                                       'sec-ch-ua-platform': 'macOS',
                                       'sec-fetch-site': 'same-origin',
                                       'sec-fetch-mode': 'cors',
                                       'sec-fetch-dest': 'empty',
                                       'referer': f'https://{SEHUATANG_HOST}/plugin.php?id=dd_sign&mod=sign',
                                       'accept-language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                                   }, *args, **kwargs)
                try:
                    r.raise_for_status()
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤ç­¾åˆ°ï¼Œä½†åœ¨æ­¤å¤„ä¸ç›´æ¥è¿”å›
                    yield r
                finally:
                    r.close()

            with client:
                age_confirmed = False
                age_retry_cnt = 3
                while not age_confirmed and age_retry_cnt > 0:
                    with _request(method='get', url=f'https://{SEHUATANG_HOST}/') as r:
                        # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                        if 'é‡å¤ç­¾åˆ°' in r.text:
                            return 'é‡å¤ç­¾åˆ°'
                        if (v := re.findall(r"safeid='(\w+)'",
                                            r.text, re.MULTILINE | re.IGNORECASE)) and (safeid := v[0]):
                            print(f'set age confirm cookie: _safe={safeid}')
                            client.cookies.set(name='_safe', value=safeid)
                        else:
                            age_confirmed = True
                        age_retry_cnt -= 1

                if not age_confirmed:
                    raise Exception('failed to pass age confirmation')

                # éšæœºç­‰å¾…
                wait_time = random.randint(1, 3)
                print(f'åœ¨é€‰æ‹©é¢˜ç›®ä¹‹å‰ç­‰å¾… {wait_time} ç§’...')
                time.sleep(wait_time)
                # éšæœºç­‰å¾…

                with _request(method='get', url=f'https://{SEHUATANG_HOST}/forum.php?mod=forumdisplay&fid={FID}') as r:
                    # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                    if 'é‡å¤ç­¾åˆ°' in r.text:
                        return 'é‡å¤ç­¾åˆ°'
                    tids = re.findall(r"normalthread_(\d+)", r.text,
                                      re.MULTILINE | re.IGNORECASE)
                    tid = random.choice(tids)
                    print(f'choose tid = {tid} to comment')

                # éšæœºç­‰å¾…
                wait_time = random.randint(1, 3)
                print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
                time.sleep(wait_time)
                # éšæœºç­‰å¾…

                with _request(method='get', url=f'https://{SEHUATANG_HOST}/forum.php?mod=viewthread&tid={tid}&extra=page%3D1') as r:
                    # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                    if 'é‡å¤ç­¾åˆ°' in r.text:
                        return 'é‡å¤ç­¾åˆ°'
                    soup = BeautifulSoup(r.text, 'html.parser')
                    formhash = soup.find('input', {'name': 'formhash'})['value']

                message = random.choice(AUTO_REPLIES)

                # éšæœºç­‰å¾…
                wait_time = random.randint(1, 3)
                print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
                time.sleep(wait_time)
                # éšæœºç­‰å¾…
                with _request(method='post', url=f'https://{SEHUATANG_HOST}/forum.php?mod=post&action=reply&fid={FID}&tid={tid}&extra=page%3D1&replysubmit=yes&infloat=yes&handlekey=fastpost&inajax=1',
                              data={
                                  'file': '',
                                  'message': message,
                                  'posttime': int(time.time()),
                                  'formhash': formhash,
                                  'usesig': '',
                                  'subject': '',
                              }) as r:
                    # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                    if 'é‡å¤ç­¾åˆ°' in r.text:
                        return 'é‡å¤ç­¾åˆ°'
                    print(f'comment to: tid = {tid}, message = {message}')

                with _request(method='get', url=f'https://{SEHUATANG_HOST}/plugin.php?id=dd_sign&mod=sign') as r:
                    # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                    if 'é‡å¤ç­¾åˆ°' in r.text:
                        return 'é‡å¤ç­¾åˆ°'
                    # id_hash_rsl = re.findall(
                    #     r"updatesecqaa\('(.*?)'", r.text, re.MULTILINE | re.IGNORECASE)
                    # id_hash = id_hash_rsl[0] if id_hash_rsl else 'qS0'  # default value

                    # soup = BeautifulSoup(r.text, 'html.parser')
                    # formhash = soup.find('input', {'name': 'formhash'})['value']
                    # signtoken = soup.find('input', {'name': 'signtoken'})['value']
                    # action = soup.find('form', {'name': 'login'})['action']
                    pass

                # éšæœºç­‰å¾…
                wait_time = random.randint(1, 3)
                print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
                time.sleep(wait_time)
                # éšæœºç­‰å¾…
                with _request(method='get', url=f'https://{SEHUATANG_HOST}/plugin.php?id=dd_sign&ac=sign&infloat=yes&handlekey=pc_click_ddsign&inajax=1&ajaxtarget=fwin_content_pc_click_ddsign') as r:
                    # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                    if 'é‡å¤ç­¾åˆ°' in r.text:
                        return 'é‡å¤ç­¾åˆ°'
                    soup = BeautifulSoup(r.text, 'xml')
                    html = soup.find('root').string
                    # extract argument values from signform
                    root = BeautifulSoup(html, 'html.parser')
                    id_hash = (root.find('span', id=re.compile(r'^secqaa_'))
                               ['id']).removeprefix('secqaa_')
                    formhash = root.find('input', {'name': 'formhash'})['value']
                    signtoken = root.find('input', {'name': 'signtoken'})['value']
                    action = root.find('form', {'name': 'login'})['action']
                    print(
                        f'signform values: id_hash={id_hash}, formhash={formhash}, signtoken={signtoken}')
                    print(f'action href: {action}')

                # GET: https://www.sehuatang.net/misc.php?mod=secqaa&action=update&idhash=qS0&0.2010053552105764
                with _request(method='get', url=f'https://{SEHUATANG_HOST}/misc.php?mod=secqaa&action=update&idhash={id_hash}&{round(random.random(), 16)}') as r:
                    # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                    if 'é‡å¤ç­¾åˆ°' in r.text:
                        return 'é‡å¤ç­¾åˆ°'
                    qes_rsl = re.findall(r"'(.*?) = \?'", r.text,
                                         re.MULTILINE | re.IGNORECASE)

                    if not qes_rsl or not qes_rsl[0]:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤ç­¾åˆ°
                        if 'é‡å¤ç­¾åˆ°' in r.text:
                            return 'é‡å¤ç­¾åˆ°'
                        raise Exception('invalid or empty question!')
                    qes = qes_rsl[0]
                    ans = eval(qes)
                    print(f'verification question: {qes} = {ans}')
                    assert type(ans) == int

                # éšæœºç­‰å¾…
                wait_time = random.randint(1, 3)
                print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
                time.sleep(wait_time)
                # éšæœºç­‰å¾…
                # POST: https://www.sehuatang.net/plugin.php?id=dd_sign&mod=sign&signsubmit=yes&signhash=LMAB9&inajax=1
                with _request(method='post', url=f'https://{SEHUATANG_HOST}/{action.lstrip("/")}&inajax=1',
                              data={'formhash': formhash,
                                    'signtoken': signtoken,
                                    'secqaahash': id_hash,
                                    'secanswer': ans}) as r:
                    # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                    if 'é‡å¤ç­¾åˆ°' in r.text:
                        return 'é‡å¤ç­¾åˆ°'
                    if 'ç­¾åˆ°æˆåŠŸ' in r.text:
                        output = f"ç­¾åˆ°æˆåŠŸ\n\n"
                        #è·å–ä¸ªäººè´¦å·å
                        with _request(method='get', url=f'https://{SEHUATANG_HOST}/home.php?mod=spacecp&ac=profile') as r:
                            # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                            if 'é‡å¤ç­¾åˆ°' in r.text:
                                return 'é‡å¤ç­¾åˆ°'
                            soup = BeautifulSoup(r.text, 'html.parser')
                            # æŸ¥æ‰¾æ ‡ç­¾
                            username_element = soup.find('strong', class_='vwmy')
                            # æå–ç”¨æˆ·ä¿¡æ¯
                            if username_element:
                                username = username_element.text
                                # æ©ç å¤„ç†ï¼šä¿ç•™é¦–å­—æ¯å’Œæœ€åä¸€å­—æ¯ï¼Œå…¶ä»–éƒ¨åˆ†ç”¨æ˜Ÿå·ä»£æ›¿
                                def mask_username(username):
                                    if len(username) <= 2:
                                        # å¦‚æœç”¨æˆ·ååªæœ‰ä¸€ä¸¤ä½ï¼Œæ©ç ä¸€åŠæˆ–å®Œå…¨æ©ç›–
                                        return username[0] + '*' * (len(username) - 1)
                                    else:
                                        return username[0] + '*' * (len(username) - 2) + username[-1]
                                masked_username = mask_username(username)
                                print(f"ç”¨æˆ·å: {masked_username}")
                            else:
                                print("æœªæ‰¾åˆ°ç”¨æˆ·å")
                            # è¾“å‡ºç»“æœ
                            output += f"ç”¨æˆ·å: {masked_username} \n"
                        #è·å–é‡‘å¸ç­‰ä¿¡æ¯
                        with _request(method='get', url=f'https://{SEHUATANG_HOST}/home.php?mod=spacecp&ac=credit&showcredit=1') as r:
                            # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                            if 'é‡å¤ç­¾åˆ°' in r.text:
                                return 'é‡å¤ç­¾åˆ°'
                            soup = BeautifulSoup(r.text, 'html.parser')
                            # æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰ <li> æ ‡ç­¾çš„ç§¯åˆ†ä¿¡æ¯
                            credit_info = soup.find_all('li')
                            # æå–ç§¯åˆ†å’Œé‡‘é’±ç­‰ä¿¡æ¯
                            credits = {}
                            first_integral_found = False
                            for item in credit_info:
                                if item.find('em'):  # æ£€æŸ¥æ˜¯å¦æœ‰ <em> æ ‡ç­¾
                                    key = item.find('em').text.strip().replace(':', '')
                                    value = item.text.split(":")[-1].strip()
                                    if key == "ç§¯åˆ†" and not first_integral_found:
                                        credits[key] = value
                                        first_integral_found = True
                                    elif key!= "ç§¯åˆ†":
                                        credits[key] = value
                            # è·å–ç”¨æˆ·ç»„ä¿¡æ¯
                            user_group_tag = soup.find('a', {'id': 'g_upmine'})
                            user_group = user_group_tag.text.split(":")[-1].strip() if user_group_tag else 'æœªæ‰¾åˆ°'
                            # è·å–å„ç±»ç§¯åˆ†ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œè¾“å‡ºæ ¼å¼åŒ–
                            gold = credits.get('é‡‘é’±', 'æœªæ‰¾åˆ°')
                            points = credits.get('ç§¯åˆ†', 'æœªæ‰¾åˆ°')
                            coins = credits.get('è‰²å¸', 'æœªæ‰¾åˆ°')
                            ratings = credits.get('è¯„åˆ†', 'æœªæ‰¾åˆ°')
                            # è¾“å‡ºç»“æœ
                            output += f"ğŸ¤¼â€â™‚ï¸ç”¨æˆ·ç»„ï¼š{user_group} \nğŸ’°é‡‘å¸ï¼š{gold}\nğŸ¥‡ç§¯åˆ†ï¼š{points}\nğŸ˜è‰²å¸ï¼š{coins}\nğŸ”¥è¯„åˆ†ï¼š{ratings}\n"
                        #è·å–å‡çº§è¯¦æƒ…
                        # éšæœºç­‰å¾…
                        wait_time = random.randint(1, 3)
                        print(f'éšæœºç­‰å¾… {wait_time} ç§’...')
                        time.sleep(wait_time)
                        # éšæœºç­‰å¾…
                        with _request(method='get', url=f'https://{SEHUATANG_HOST}/home.php?mod=spacecp&ac=usergroup') as r:
                            # æ£€æŸ¥æ˜¯å¦é‡å¤ç­¾åˆ°
                            if 'é‡å¤ç­¾åˆ°' in r.text:
                                return 'é‡å¤ç­¾åˆ°'
                            soup = BeautifulSoup(r.text, 'html.parser')
                            # è·å–æ™‹çº§ç”¨æˆ·ç»„ä¿¡æ¯
                            upgrade_usergroup = soup.find('li', {'id': 'c2'}).text
                            #print("æ™‹çº§ç”¨æˆ·ç»„ä¿¡æ¯:", upgrade_usergroup)
                            # æŸ¥æ‰¾åŒ…å«æ™‹çº§ç”¨æˆ·ç»„ä¿¡æ¯çš„div
                            tscr_div = soup.find('div', class_='tscr')
                            if tscr_div:
                                # æå–å‡çº§æ‰€éœ€ç§¯åˆ†ä¿¡æ¯
                                required_points = tscr_div.find('span', class_='notice').text
                                # ä»æ–‡æœ¬ä¸­æå–ç§¯åˆ†æ•°å­—
                                points_needed = int(required_points.split('ç§¯åˆ†')[1].strip())
                                # è®¡ç®—é¢„è®¡å‡çº§æ—¶é—´
                                points_per_day = 3
                                days_needed = points_needed / points_per_day
                                # æ‰‹åŠ¨å‘ä¸Šå–æ•´
                                if days_needed != int(days_needed):
                                    days_needed_ceil = int(days_needed) + 1
                                else:
                                    days_needed_ceil = int(days_needed)
                                # è®¡ç®—é¢„è®¡å‡çº§æ—¥æœŸ
                                today = datetime.now()
                                todayDate = today.date()  
                                upgrade_date = todayDate + timedelta(days=days_needed_ceil)
                            else:
                                print("æœªæ‰¾åˆ°åŒ…å«æ™‹çº§ç”¨æˆ·ç»„ä¿¡æ¯çš„ divã€‚")
                            # è¾“å‡ºç»“æœ
                            output += f"\n{upgrade_usergroup} \n{required_points}\n"
                            output += f"é¢„è®¡è¿˜éœ€{days_needed_ceil}å¤©\n"
                            output += f"é¢„è®¡å‡çº§æ—¶é—´: {upgrade_date}"
                        return output
                    elif 'é‡å¤ç­¾åˆ°' in r.text:
                        return 'é‡å¤ç­¾åˆ°'
                    elif 'éœ€è¦å…ˆç™»å½•' in r.text:
                        return 'éœ€è¦å…ˆç™»å½•'
                    else:
                        return r.text

        except httpx.ProxyError as e:
            print(f"ä»£ç†é”™è¯¯: {str(e)}")
            if current_proxy:
                print(f"å½“å‰ä»£ç† {current_proxy} ä¸å¯ç”¨ï¼Œå°è¯•åˆ‡æ¢ä»£ç†...")
                # å°è¯•ä½¿ç”¨æ–°çš„ä»£ç†
                available_proxies = [p for p in proxy_pool if p not in used_proxies]
                if available_proxies:
                    proxy_url = random.choice(available_proxies)
                    current_proxy = create_proxy_dict(proxy_url)
                    used_proxies.add(proxy_url)
                    print(f"åˆ‡æ¢åˆ°æ–°ä»£ç†: {current_proxy}")
                    time.sleep(retry_delay)
                    continue
                else:
                    print("æ‰€æœ‰ä»£ç†éƒ½å·²å°è¯•è¿‡ï¼Œæ— æ³•ç»§ç»­")
                    raise Exception("æ‰€æœ‰ä»£ç†éƒ½ä¸å¯ç”¨")
            else:
                raise Exception(f"ä»£ç†é”™è¯¯: {str(e)}")

        except httpx.RequestError as e:
            print(f"è¯·æ±‚é”™è¯¯: {str(e)}")
            if retry < max_retries - 1:
                print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                continue
            else:
                raise Exception(f"è¯·æ±‚é”™è¯¯: {str(e)}")

        except Exception as e:
            print(f"æœªçŸ¥é”™è¯¯: {str(e)}")
            traceback.print_exc()
            if retry < max_retries - 1:
                print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                continue
            else:
                raise Exception(f"æœªçŸ¥é”™è¯¯: {str(e)}")

def retrieve_cookies_from_curl(env: str) -> dict:
    cURL = os.getenv(env, '').replace('\\', ' ')
    try:
        import uncurl
        return uncurl.parse_context(curl_command=cURL).cookies
    except ImportError:
        print("uncurl is required.")


def retrieve_cookies_from_fetch(env: str) -> dict:
    def parse_fetch(s: str) -> dict:
        ans = {}
        exec(s, {
            'fetch': lambda _, o: ans.update(o),
            'null': None
        })
        return ans
    cookie_str = parse_fetch(os.getenv(env))['headers']['cookie']
    return dict(s.strip().split('=', maxsplit=1) for s in cookie_str.split(';'))


def preprocess_text(text) -> str:
    if 'xml' not in text:
        return text

    try:
        root = ET.fromstring(text)
        cdata = root.text
        soup = BeautifulSoup(cdata, 'html.parser')
        for script in soup.find_all('script'):
            script.decompose()
        return soup.get_text()
    except:
        return text


def push_notification(title: str, content: str) -> None:

    def telegram_send_message(text: str, chat_id: str, token: str, silent: bool = False) -> None:
        r = httpx.post(url=f'https://api.telegram.org/bot{token}/sendMessage',
                       json={
                           'chat_id': chat_id,
                           'text': text,
                           'disable_notification': silent,
                           'disable_web_page_preview': True,
                       })
        r.raise_for_status()

    try:
        from notify import telegram_bot
        telegram_bot(title, content)
    except ImportError:
        chat_id = os.getenv('TG_USER_ID')
        bot_token = os.getenv('TG_BOT_TOKEN')
        if chat_id and bot_token:
            telegram_send_message(f'{title}\n\n{content}', chat_id, bot_token)


def main():
    raw_html = None
    results = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰ç­¾åˆ°ç»“æœ

    # éå†å¤šä¸ª FETCH_98TANG ç¯å¢ƒå˜é‡
    fetch_index = 1
    while os.getenv(f'FETCH_98TANG_{fetch_index}'):
        try:
            # è·å– cookies
            cookies = retrieve_cookies_from_fetch(f'FETCH_98TANG_{fetch_index}')
            
            # è·å–å¯¹åº”çš„ä»£ç†
            proxy = os.getenv(f'DEAULT_PROXY_{fetch_index}', None)
            if proxy:
                proxies = {'http': proxy, 'https': proxy}
                print(f"è´¦å· {fetch_index} ä½¿ç”¨ç¯å¢ƒå˜é‡ä»£ç†: {proxies}")
            else:
                proxies = None
                # ç§»é™¤é”™è¯¯æç¤ºï¼Œç›´æ¥ä½¿ç”¨ä»£ç†æ± 
                print(f"è´¦å· {fetch_index} ä½¿ç”¨ä»£ç†æ± ")

            # æ‰§è¡Œç­¾åˆ°ï¼Œå¹¶ä¼ é€’ä»£ç†ä¿¡æ¯
            raw_html = daysign(
                cookies=cookies,
                flaresolverr_url=os.getenv('FLARESOLVERR_URL'),
                flaresolverr_proxy=os.getenv('FLARESOLVERR_PROXY'),
                proxies=proxies
            )

            if 'ç­¾åˆ°æˆåŠŸ' in raw_html:
                title = f'ç¬¬{fetch_index}ä¸ªè´¦å·'
                message_text = raw_html
            elif raw_html == 'é‡å¤ç­¾åˆ°':
                title = f'ç¬¬{fetch_index}ä¸ªè´¦å·'
                message_text = 'æœ¬æ—¥å·²ç­¾åˆ°ï¼Œè¯·å‹¿é‡å¤ç­¾åˆ°'
            elif raw_html == 'éœ€è¦å…ˆç™»å½•':
                title = f'ç¬¬{fetch_index}ä¸ªè´¦å· ç­¾åˆ°å¼‚å¸¸'
                message_text = 'Cookieæ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·é‡æ–°è·å–'
            else:
                title = f'ç¬¬{fetch_index}ä¸ªè´¦å· ç­¾åˆ°å¼‚å¸¸'
                message_text = raw_html

        except Exception as e:
            title = f'ç¬¬{fetch_index}ä¸ªè´¦å· ç­¾åˆ°å¼‚å¸¸'
            message_text = f'é”™è¯¯åŸå› ï¼š{str(e)}\nè¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š{traceback.format_exc()}'
            print(f"è´¦å· {fetch_index} ç­¾åˆ°å¤±è´¥: {message_text}")

        # å¤„ç†å¹¶ä¿å­˜ç»“æœ
        message_text = preprocess_text(message_text)
        results.append(f'{title}\n{message_text}')
        
        fetch_index += 1

    # è¾“å‡ºæ‰€æœ‰ç»“æœ
    for result in results:
        print(result)

    #é’é¾™é€šçŸ¥
    full_message = "\n\n".join(results)
    from notify import send
    send('ğŸ—“98å ‚æ¯æ—¥ç­¾åˆ°', full_message)

if __name__ == '__main__':
    main()
